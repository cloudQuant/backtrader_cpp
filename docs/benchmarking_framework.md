# æ€§èƒ½åŸºå‡†æµ‹è¯•æ¡†æ¶è®¾è®¡

æœ¬æ–‡æ¡£è®¾è®¡äº†ä¸€ä¸ªå…¨é¢çš„æ€§èƒ½åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œç”¨äºéªŒè¯C++é‡æ„ç‰ˆæœ¬çš„æ€§èƒ½æå‡æ•ˆæœï¼Œå¹¶æä¾›æŒç»­çš„æ€§èƒ½ç›‘æ§èƒ½åŠ›ã€‚

## ğŸ¯ æµ‹è¯•æ¡†æ¶ç›®æ ‡

### æ ¸å¿ƒç›®æ ‡
1. **å‡†ç¡®æ€§**: ç²¾ç¡®æµ‹é‡å„ç»„ä»¶çš„æ€§èƒ½å·®å¼‚
2. **å…¨é¢æ€§**: è¦†ç›–æ‰€æœ‰å…³é”®æ€§èƒ½åœºæ™¯
3. **å¯é‡å¤æ€§**: ç¡®ä¿æµ‹è¯•ç»“æœç¨³å®šå¯é 
4. **å¯æ¯”æ€§**: Pythonç‰ˆæœ¬ä¸C++ç‰ˆæœ¬çš„ç›´æ¥å¯¹æ¯”
5. **æŒç»­æ€§**: æ”¯æŒCI/CDé›†æˆçš„æ€§èƒ½å›å½’æ£€æµ‹

### æµ‹è¯•ç»´åº¦
- **ç»„ä»¶çº§åˆ«**: å•ä¸ªæŒ‡æ ‡ã€ç­–ç•¥ã€æ•°æ®å¤„ç†ç»„ä»¶
- **ç³»ç»Ÿçº§åˆ«**: å®Œæ•´å›æµ‹æµç¨‹ã€å¤šç­–ç•¥å¹¶è¡Œ
- **åœºæ™¯çº§åˆ«**: ä¸åŒæ•°æ®è§„æ¨¡ã€ä¸åŒå¤æ‚åº¦ç­–ç•¥
- **èµ„æºçº§åˆ«**: CPUä½¿ç”¨ç‡ã€å†…å­˜å ç”¨ã€I/Oæ€§èƒ½

## ğŸ—ï¸ æ¡†æ¶æ¶æ„è®¾è®¡

### æ•´ä½“æ¶æ„

```
åŸºå‡†æµ‹è¯•æ¡†æ¶
â”œâ”€â”€ æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨ (DataGenerator)
â”œâ”€â”€ æ€§èƒ½æµ‹è¯•æ‰§è¡Œå™¨ (BenchmarkExecutor)
â”œâ”€â”€ ç»“æœæ”¶é›†å™¨ (ResultCollector)
â”œâ”€â”€ æ€§èƒ½åˆ†æå™¨ (PerformanceAnalyzer)
â””â”€â”€ æŠ¥å‘Šç”Ÿæˆå™¨ (ReportGenerator)
```

### æ ¸å¿ƒç»„ä»¶è®¾è®¡

#### 1. åŸºå‡†æµ‹è¯•åŸºç±»

```cpp
// BenchmarkBase.h
#pragma once
#include <chrono>
#include <string>
#include <vector>
#include <memory>
#include <functional>

struct BenchmarkResult {
    std::string name;
    double execution_time_ns;
    double memory_usage_mb;
    double cpu_usage_percent;
    size_t iterations;
    std::map<std::string, double> custom_metrics;
    
    // ç»Ÿè®¡ä¿¡æ¯
    double mean_time;
    double median_time;
    double std_dev_time;
    double min_time;
    double max_time;
};

struct BenchmarkConfig {
    size_t min_iterations = 10;
    size_t max_iterations = 1000;
    double min_time_seconds = 1.0;
    double max_time_seconds = 60.0;
    bool warmup_enabled = true;
    size_t warmup_iterations = 3;
    bool memory_tracking = true;
    bool cpu_tracking = true;
};

class BenchmarkBase {
protected:
    std::string name_;
    BenchmarkConfig config_;
    std::vector<double> execution_times_;
    
public:
    explicit BenchmarkBase(const std::string& name, 
                          const BenchmarkConfig& config = {})
        : name_(name), config_(config) {}
    
    virtual ~BenchmarkBase() = default;
    
    // çº¯è™šå‡½æ•° - å­ç±»å®ç°å…·ä½“æµ‹è¯•é€»è¾‘
    virtual void SetUp() = 0;
    virtual void Execute() = 0;
    virtual void TearDown() = 0;
    
    // è¿è¡ŒåŸºå‡†æµ‹è¯•
    BenchmarkResult Run();
    
    // è‡ªå®šä¹‰æŒ‡æ ‡
    virtual std::map<std::string, double> GetCustomMetrics() { return {}; }
    
protected:
    // å†…å­˜ä½¿ç”¨ç›‘æ§
    double GetMemoryUsageMB();
    
    // CPUä½¿ç”¨ç‡ç›‘æ§
    double GetCpuUsagePercent();
};

// å®ç°
BenchmarkResult BenchmarkBase::Run() {
    SetUp();
    
    // é¢„çƒ­
    if (config_.warmup_enabled) {
        for (size_t i = 0; i < config_.warmup_iterations; ++i) {
            Execute();
        }
    }
    
    execution_times_.clear();
    
    auto start_memory = GetMemoryUsageMB();
    auto start_time = std::chrono::steady_clock::now();
    
    size_t iterations = 0;
    double total_time = 0.0;
    
    // è‡ªé€‚åº”è¿­ä»£æ¬¡æ•°
    while (iterations < config_.min_iterations || 
           (total_time < config_.min_time_seconds && iterations < config_.max_iterations)) {
        
        auto iter_start = std::chrono::high_resolution_clock::now();
        Execute();
        auto iter_end = std::chrono::high_resolution_clock::now();
        
        auto duration = std::chrono::duration_cast<std::chrono::nanoseconds>(
            iter_end - iter_start).count();
        
        execution_times_.push_back(static_cast<double>(duration));
        total_time = std::chrono::duration<double>(iter_end - start_time).count();
        iterations++;
        
        if (total_time > config_.max_time_seconds) break;
    }
    
    auto end_memory = GetMemoryUsageMB();
    auto cpu_usage = GetCpuUsagePercent();
    
    TearDown();
    
    // è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    std::sort(execution_times_.begin(), execution_times_.end());
    
    double mean = std::accumulate(execution_times_.begin(), execution_times_.end(), 0.0) 
                  / execution_times_.size();
    double median = execution_times_[execution_times_.size() / 2];
    
    double variance = 0.0;
    for (double time : execution_times_) {
        variance += (time - mean) * (time - mean);
    }
    double std_dev = std::sqrt(variance / execution_times_.size());
    
    BenchmarkResult result;
    result.name = name_;
    result.execution_time_ns = mean;
    result.memory_usage_mb = end_memory - start_memory;
    result.cpu_usage_percent = cpu_usage;
    result.iterations = iterations;
    result.mean_time = mean;
    result.median_time = median;
    result.std_dev_time = std_dev;
    result.min_time = execution_times_.front();
    result.max_time = execution_times_.back();
    result.custom_metrics = GetCustomMetrics();
    
    return result;
}
```

#### 2. æŒ‡æ ‡æ€§èƒ½æµ‹è¯•

```cpp
// IndicatorBenchmarks.h
#pragma once
#include "BenchmarkBase.h"
#include "../indicators/SMA.h"
#include "../indicators/EMA.h"
#include "../indicators/RSI.h"
#include "../core/LineRoot.h"

// SMAåŸºå‡†æµ‹è¯•
class SMABenchmark : public BenchmarkBase {
private:
    std::vector<double> test_data_;
    std::shared_ptr<LineRoot> data_line_;
    std::unique_ptr<SMA> sma_;
    size_t period_;
    size_t data_size_;
    
public:
    SMABenchmark(size_t period, size_t data_size)
        : BenchmarkBase("SMA_" + std::to_string(period) + "_" + std::to_string(data_size))
        , period_(period), data_size_(data_size) {}
    
    void SetUp() override {
        // ç”Ÿæˆæµ‹è¯•æ•°æ®
        test_data_ = GenerateRandomWalk(data_size_, 100.0, 0.02);
        
        // åˆ›å»ºæ•°æ®çº¿
        data_line_ = std::make_shared<LineRoot>();
        for (double value : test_data_) {
            data_line_->forward(value);
        }
        
        // åˆ›å»ºSMAæŒ‡æ ‡
        sma_ = std::make_unique<SMA>(data_line_, period_);
    }
    
    void Execute() override {
        sma_->calculate();
    }
    
    void TearDown() override {
        sma_.reset();
        data_line_.reset();
        test_data_.clear();
    }
    
    std::map<std::string, double> GetCustomMetrics() override {
        return {
            {"throughput_bars_per_second", static_cast<double>(data_size_) / (execution_times_.back() / 1e9)},
            {"period", static_cast<double>(period_)},
            {"data_size", static_cast<double>(data_size_)}
        };
    }
    
private:
    std::vector<double> GenerateRandomWalk(size_t size, double initial, double volatility) {
        std::vector<double> data;
        data.reserve(size);
        
        std::random_device rd;
        std::mt19937 gen(rd());
        std::normal_distribution<double> dist(0.0, volatility);
        
        double current = initial;
        for (size_t i = 0; i < size; ++i) {
            current *= (1.0 + dist(gen));
            data.push_back(current);
        }
        
        return data;
    }
};

// EMAåŸºå‡†æµ‹è¯•
class EMABenchmark : public BenchmarkBase {
private:
    std::vector<double> test_data_;
    std::shared_ptr<LineRoot> data_line_;
    std::unique_ptr<EMA> ema_;
    size_t period_;
    size_t data_size_;
    
public:
    EMABenchmark(size_t period, size_t data_size)
        : BenchmarkBase("EMA_" + std::to_string(period) + "_" + std::to_string(data_size))
        , period_(period), data_size_(data_size) {}
    
    void SetUp() override {
        test_data_ = GenerateRandomWalk(data_size_, 100.0, 0.02);
        
        data_line_ = std::make_shared<LineRoot>();
        for (double value : test_data_) {
            data_line_->forward(value);
        }
        
        ema_ = std::make_unique<EMA>(data_line_, period_);
    }
    
    void Execute() override {
        ema_->calculate();
    }
    
    void TearDown() override {
        ema_.reset();
        data_line_.reset();
        test_data_.clear();
    }
    
    std::map<std::string, double> GetCustomMetrics() override {
        return {
            {"throughput_bars_per_second", static_cast<double>(data_size_) / (execution_times_.back() / 1e9)},
            {"period", static_cast<double>(period_)},
            {"alpha", ema_->getAlpha()}
        };
    }
};

// å¤æ‚æŒ‡æ ‡åŸºå‡†æµ‹è¯•
class ComplexIndicatorBenchmark : public BenchmarkBase {
private:
    std::vector<double> test_data_;
    std::shared_ptr<LineRoot> data_line_;
    std::vector<std::unique_ptr<IndicatorBase>> indicators_;
    size_t data_size_;
    
public:
    explicit ComplexIndicatorBenchmark(size_t data_size)
        : BenchmarkBase("ComplexIndicators_" + std::to_string(data_size))
        , data_size_(data_size) {}
    
    void SetUp() override {
        test_data_ = GenerateRandomWalk(data_size_, 100.0, 0.02);
        
        data_line_ = std::make_shared<LineRoot>();
        for (double value : test_data_) {
            data_line_->forward(value);
        }
        
        // åˆ›å»ºå¤šä¸ªæŒ‡æ ‡
        indicators_.push_back(std::make_unique<SMA>(data_line_, 20));
        indicators_.push_back(std::make_unique<EMA>(data_line_, 20));
        indicators_.push_back(std::make_unique<RSI>(data_line_, 14));
        indicators_.push_back(std::make_unique<BollingerBands>(data_line_, 20, 2.0));
        indicators_.push_back(std::make_unique<MACD>(data_line_, 12, 26, 9));
    }
    
    void Execute() override {
        // å¹¶è¡Œè®¡ç®—æ‰€æœ‰æŒ‡æ ‡
        #pragma omp parallel for
        for (auto& indicator : indicators_) {
            indicator->calculate();
        }
    }
    
    void TearDown() override {
        indicators_.clear();
        data_line_.reset();
        test_data_.clear();
    }
    
    std::map<std::string, double> GetCustomMetrics() override {
        return {
            {"indicator_count", static_cast<double>(indicators_.size())},
            {"total_throughput", static_cast<double>(data_size_ * indicators_.size()) / (execution_times_.back() / 1e9)}
        };
    }
};
```

#### 3. ç­–ç•¥æ€§èƒ½æµ‹è¯•

```cpp
// StrategyBenchmarks.h
#pragma once
#include "BenchmarkBase.h"
#include "../strategy/StrategyBase.h"
#include "../engine/Cerebro.h"

class SimpleStrategyBenchmark : public BenchmarkBase {
private:
    std::unique_ptr<Cerebro> cerebro_;
    std::vector<OHLCV> test_data_;
    size_t data_size_;
    
public:
    explicit SimpleStrategyBenchmark(size_t data_size)
        : BenchmarkBase("SimpleStrategy_" + std::to_string(data_size))
        , data_size_(data_size) {}
    
    void SetUp() override {
        // ç”ŸæˆOHLCVæµ‹è¯•æ•°æ®
        test_data_ = GenerateOHLCVData(data_size_);
        
        // åˆ›å»ºCerebroå®ä¾‹
        cerebro_ = std::make_unique<Cerebro>();
        
        // æ·»åŠ æ•°æ®
        auto data_feed = std::make_shared<VectorDataFeed>(test_data_);
        cerebro_->addData(data_feed);
        
        // æ·»åŠ ç®€å•ç­–ç•¥
        cerebro_->addStrategy<SimpleSMAStrategy>();
    }
    
    void Execute() override {
        cerebro_->run();
        cerebro_->reset();  // é‡ç½®ä»¥ä¾¿ä¸‹æ¬¡è¿è¡Œ
    }
    
    void TearDown() override {
        cerebro_.reset();
        test_data_.clear();
    }
    
    std::map<std::string, double> GetCustomMetrics() override {
        return {
            {"bars_per_second", static_cast<double>(data_size_) / (execution_times_.back() / 1e9)},
            {"data_size", static_cast<double>(data_size_)}
        };
    }
    
private:
    std::vector<OHLCV> GenerateOHLCVData(size_t size) {
        std::vector<OHLCV> data;
        data.reserve(size);
        
        std::random_device rd;
        std::mt19937 gen(rd());
        std::normal_distribution<double> price_change(0.0, 0.02);
        std::uniform_real_distribution<double> volume_dist(1000, 10000);
        
        double base_price = 100.0;
        auto start_time = std::chrono::system_clock::now();
        
        for (size_t i = 0; i < size; ++i) {
            OHLCV bar;
            bar.datetime = start_time + std::chrono::hours(24 * i);
            
            double change = price_change(gen);
            double open = base_price * (1.0 + change);
            double high = open * (1.0 + std::abs(price_change(gen)));
            double low = open * (1.0 - std::abs(price_change(gen)));
            double close = open + (high - low) * (gen() % 1000) / 1000.0;
            
            bar.open = open;
            bar.high = high;
            bar.low = low;
            bar.close = close;
            bar.volume = volume_dist(gen);
            
            data.push_back(bar);
            base_price = close;
        }
        
        return data;
    }
};

// å¤æ‚ç­–ç•¥åŸºå‡†æµ‹è¯•
class ComplexStrategyBenchmark : public BenchmarkBase {
private:
    std::unique_ptr<Cerebro> cerebro_;
    std::vector<OHLCV> test_data_;
    size_t data_size_;
    size_t strategy_count_;
    
public:
    ComplexStrategyBenchmark(size_t data_size, size_t strategy_count)
        : BenchmarkBase("ComplexStrategy_" + std::to_string(data_size) + "_" + std::to_string(strategy_count))
        , data_size_(data_size), strategy_count_(strategy_count) {}
    
    void SetUp() override {
        test_data_ = GenerateOHLCVData(data_size_);
        
        cerebro_ = std::make_unique<Cerebro>();
        
        auto data_feed = std::make_shared<VectorDataFeed>(test_data_);
        cerebro_->addData(data_feed);
        
        // æ·»åŠ å¤šä¸ªå¤æ‚ç­–ç•¥
        for (size_t i = 0; i < strategy_count_; ++i) {
            cerebro_->addStrategy<ComplexMultiIndicatorStrategy>();
        }
    }
    
    void Execute() override {
        cerebro_->run();
        cerebro_->reset();
    }
    
    void TearDown() override {
        cerebro_.reset();
        test_data_.clear();
    }
    
    std::map<std::string, double> GetCustomMetrics() override {
        return {
            {"strategy_count", static_cast<double>(strategy_count_)},
            {"total_bars_processed", static_cast<double>(data_size_ * strategy_count_)},
            {"throughput", static_cast<double>(data_size_ * strategy_count_) / (execution_times_.back() / 1e9)}
        };
    }
};
```

#### 4. å†…å­˜å’Œç¼“å­˜æ€§èƒ½æµ‹è¯•

```cpp
// MemoryBenchmarks.h
#pragma once
#include "BenchmarkBase.h"

class MemoryAccessBenchmark : public BenchmarkBase {
private:
    std::vector<double> sequential_data_;
    std::vector<double> random_data_;
    std::vector<size_t> random_indices_;
    size_t data_size_;
    
public:
    explicit MemoryAccessBenchmark(size_t data_size)
        : BenchmarkBase("MemoryAccess_" + std::to_string(data_size))
        , data_size_(data_size) {}
    
    void SetUp() override {
        sequential_data_.resize(data_size_);
        random_data_.resize(data_size_);
        random_indices_.resize(data_size_);
        
        // åˆå§‹åŒ–æ•°æ®
        std::iota(sequential_data_.begin(), sequential_data_.end(), 0);
        std::copy(sequential_data_.begin(), sequential_data_.end(), random_data_.begin());
        std::iota(random_indices_.begin(), random_indices_.end(), 0);
        
        // éšæœºæ‰“ä¹±
        std::random_device rd;
        std::mt19937 gen(rd());
        std::shuffle(random_data_.begin(), random_data_.end(), gen);
        std::shuffle(random_indices_.begin(), random_indices_.end(), gen);
    }
    
    void Execute() override {
        // æµ‹è¯•é¡ºåºè®¿é—®vséšæœºè®¿é—®
        double sequential_sum = 0.0;
        double random_sum = 0.0;
        
        // é¡ºåºè®¿é—®
        for (size_t i = 0; i < data_size_; ++i) {
            sequential_sum += sequential_data_[i];
        }
        
        // éšæœºè®¿é—®
        for (size_t i = 0; i < data_size_; ++i) {
            random_sum += random_data_[random_indices_[i]];
        }
        
        // é˜²æ­¢ç¼–è¯‘å™¨ä¼˜åŒ–
        volatile double result = sequential_sum + random_sum;
        (void)result;
    }
    
    void TearDown() override {
        sequential_data_.clear();
        random_data_.clear();
        random_indices_.clear();
    }
};

class CacheEfficiencyBenchmark : public BenchmarkBase {
private:
    struct AoS {  // Array of Structures
        double open, high, low, close, volume;
    };
    
    struct SoA {  // Structure of Arrays
        std::vector<double> open, high, low, close, volume;
    };
    
    std::vector<AoS> aos_data_;
    SoA soa_data_;
    size_t data_size_;
    
public:
    explicit CacheEfficiencyBenchmark(size_t data_size)
        : BenchmarkBase("CacheEfficiency_" + std::to_string(data_size))
        , data_size_(data_size) {}
    
    void SetUp() override {
        aos_data_.resize(data_size_);
        soa_data_.open.resize(data_size_);
        soa_data_.high.resize(data_size_);
        soa_data_.low.resize(data_size_);
        soa_data_.close.resize(data_size_);
        soa_data_.volume.resize(data_size_);
        
        // åˆå§‹åŒ–æ•°æ®
        std::random_device rd;
        std::mt19937 gen(rd());
        std::uniform_real_distribution<double> dist(100.0, 200.0);
        
        for (size_t i = 0; i < data_size_; ++i) {
            double price = dist(gen);
            
            aos_data_[i] = {price, price * 1.01, price * 0.99, price * 1.005, 1000.0};
            
            soa_data_.open[i] = price;
            soa_data_.high[i] = price * 1.01;
            soa_data_.low[i] = price * 0.99;
            soa_data_.close[i] = price * 1.005;
            soa_data_.volume[i] = 1000.0;
        }
    }
    
    void Execute() override {
        // æµ‹è¯•AoS vs SoAçš„ç¼“å­˜æ•ˆç‡
        
        // AoSè®¿é—®æ¨¡å¼ - åªå¤„ç†closeä»·æ ¼
        double aos_sum = 0.0;
        for (size_t i = 0; i < data_size_; ++i) {
            aos_sum += aos_data_[i].close;  // ç¼“å­˜ä¸å‹å¥½ï¼Œæ¯æ¬¡åŠ è½½æ•´ä¸ªç»“æ„
        }
        
        // SoAè®¿é—®æ¨¡å¼ - åªå¤„ç†closeä»·æ ¼
        double soa_sum = 0.0;
        for (size_t i = 0; i < data_size_; ++i) {
            soa_sum += soa_data_.close[i];  // ç¼“å­˜å‹å¥½ï¼Œè¿ç»­å†…å­˜è®¿é—®
        }
        
        volatile double result = aos_sum + soa_sum;
        (void)result;
    }
    
    void TearDown() override {
        aos_data_.clear();
        soa_data_.open.clear();
        soa_data_.high.clear();
        soa_data_.low.clear();
        soa_data_.close.clear();
        soa_data_.volume.clear();
    }
};
```

#### 5. Python vs C++ å¯¹æ¯”æµ‹è¯•

```cpp
// ComparisonBenchmarks.h
#pragma once
#include "BenchmarkBase.h"

class PythonVsCppBenchmark : public BenchmarkBase {
private:
    std::string python_script_;
    std::string test_data_file_;
    size_t data_size_;
    
public:
    PythonVsCppBenchmark(const std::string& test_name, size_t data_size)
        : BenchmarkBase("PythonVsCpp_" + test_name + "_" + std::to_string(data_size))
        , data_size_(data_size) {}
    
    void SetUp() override {
        // ç”Ÿæˆæµ‹è¯•æ•°æ®æ–‡ä»¶
        test_data_file_ = "test_data_" + std::to_string(data_size_) + ".csv";
        GenerateTestDataFile(test_data_file_, data_size_);
        
        // å‡†å¤‡Pythonæµ‹è¯•è„šæœ¬
        python_script_ = R"(
import backtrader as bt
import pandas as pd
import time

class TestStrategy(bt.Strategy):
    def __init__(self):
        self.sma = bt.indicators.SMA(period=20)
        self.rsi = bt.indicators.RSI(period=14)
    
    def next(self):
        if not self.position:
            if self.data.close[0] > self.sma[0] and self.rsi[0] < 30:
                self.buy()
        else:
            if self.rsi[0] > 70:
                self.sell()

cerebro = bt.Cerebro()
data = bt.feeds.GenericCSVData(dataname=')" + test_data_file_ + R"(')
cerebro.adddata(data)
cerebro.addstrategy(TestStrategy)

start_time = time.time()
cerebro.run()
end_time = time.time()

print(f"Python execution time: {(end_time - start_time) * 1000:.2f}ms")
)";
    }
    
    void Execute() override {
        // è¿è¡ŒC++ç‰ˆæœ¬
        auto start_cpp = std::chrono::high_resolution_clock::now();
        RunCppVersion();
        auto end_cpp = std::chrono::high_resolution_clock::now();
        
        // è¿è¡ŒPythonç‰ˆæœ¬
        auto start_python = std::chrono::high_resolution_clock::now();
        RunPythonVersion();
        auto end_python = std::chrono::high_resolution_clock::now();
        
        // è®°å½•æ—¶é—´å·®å¼‚
        cpp_time_ = std::chrono::duration<double, std::milli>(end_cpp - start_cpp).count();
        python_time_ = std::chrono::duration<double, std::milli>(end_python - start_python).count();
    }
    
    void TearDown() override {
        std::remove(test_data_file_.c_str());
    }
    
    std::map<std::string, double> GetCustomMetrics() override {
        return {
            {"cpp_time_ms", cpp_time_},
            {"python_time_ms", python_time_},
            {"speedup_factor", python_time_ / cpp_time_},
            {"data_size", static_cast<double>(data_size_)}
        };
    }
    
private:
    double cpp_time_;
    double python_time_;
    
    void RunCppVersion() {
        // å®ç°C++ç‰ˆæœ¬çš„ç›¸åŒé€»è¾‘
        auto cerebro = std::make_unique<Cerebro>();
        auto data = std::make_shared<CSVDataFeed>(test_data_file_);
        cerebro->addData(data);
        cerebro->addStrategy<TestStrategy>();
        cerebro->run();
    }
    
    void RunPythonVersion() {
        // æ‰§è¡ŒPythonè„šæœ¬
        std::string command = "python3 -c \"" + python_script_ + "\"";
        std::system(command.c_str());
    }
    
    void GenerateTestDataFile(const std::string& filename, size_t size) {
        std::ofstream file(filename);
        file << "Date,Open,High,Low,Close,Volume\n";
        
        auto start_date = std::chrono::system_clock::now();
        double price = 100.0;
        
        std::random_device rd;
        std::mt19937 gen(rd());
        std::normal_distribution<double> price_change(0.0, 0.02);
        
        for (size_t i = 0; i < size; ++i) {
            auto date = start_date + std::chrono::hours(24 * i);
            auto time_t_date = std::chrono::system_clock::to_time_t(date);
            
            double change = price_change(gen);
            double open = price * (1.0 + change);
            double high = open * (1.0 + std::abs(change));
            double low = open * (1.0 - std::abs(change));
            double close = open + (high - low) * 0.5;
            
            file << std::put_time(std::gmtime(&time_t_date), "%Y-%m-%d") << ","
                 << open << "," << high << "," << low << "," << close << ",1000\n";
            
            price = close;
        }
    }
};
```

### 6. åŸºå‡†æµ‹è¯•å¥—ä»¶ç®¡ç†å™¨

```cpp
// BenchmarkSuite.h
#pragma once
#include "BenchmarkBase.h"
#include <vector>
#include <memory>
#include <fstream>
#include <iomanip>

class BenchmarkSuite {
private:
    std::vector<std::unique_ptr<BenchmarkBase>> benchmarks_;
    std::string output_file_;
    
public:
    explicit BenchmarkSuite(const std::string& output_file = "benchmark_results.json")
        : output_file_(output_file) {}
    
    // æ·»åŠ åŸºå‡†æµ‹è¯•
    template<typename T, typename... Args>
    void AddBenchmark(Args&&... args) {
        benchmarks_.push_back(std::make_unique<T>(std::forward<Args>(args)...));
    }
    
    // è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•
    std::vector<BenchmarkResult> RunAll() {
        std::vector<BenchmarkResult> results;
        results.reserve(benchmarks_.size());
        
        std::cout << "Running " << benchmarks_.size() << " benchmarks...\n";
        
        for (size_t i = 0; i < benchmarks_.size(); ++i) {
            std::cout << "[" << (i + 1) << "/" << benchmarks_.size() << "] "
                      << "Running " << benchmarks_[i]->GetName() << "..." << std::flush;
            
            auto result = benchmarks_[i]->Run();
            results.push_back(result);
            
            std::cout << " Done (" << std::fixed << std::setprecision(2)
                      << result.mean_time / 1e6 << "ms avg)\n";
        }
        
        // ä¿å­˜ç»“æœ
        SaveResults(results);
        
        return results;
    }
    
    // ä¿å­˜ç»“æœåˆ°JSONæ–‡ä»¶
    void SaveResults(const std::vector<BenchmarkResult>& results) {
        std::ofstream file(output_file_);
        file << "{\n";
        file << "  \"benchmark_results\": [\n";
        
        for (size_t i = 0; i < results.size(); ++i) {
            const auto& result = results[i];
            
            file << "    {\n";
            file << "      \"name\": \"" << result.name << "\",\n";
            file << "      \"mean_time_ns\": " << result.mean_time << ",\n";
            file << "      \"median_time_ns\": " << result.median_time << ",\n";
            file << "      \"std_dev_ns\": " << result.std_dev_time << ",\n";
            file << "      \"min_time_ns\": " << result.min_time << ",\n";
            file << "      \"max_time_ns\": " << result.max_time << ",\n";
            file << "      \"iterations\": " << result.iterations << ",\n";
            file << "      \"memory_usage_mb\": " << result.memory_usage_mb << ",\n";
            file << "      \"cpu_usage_percent\": " << result.cpu_usage_percent << ",\n";
            file << "      \"custom_metrics\": {\n";
            
            size_t metric_count = 0;
            for (const auto& [key, value] : result.custom_metrics) {
                file << "        \"" << key << "\": " << value;
                if (++metric_count < result.custom_metrics.size()) {
                    file << ",";
                }
                file << "\n";
            }
            
            file << "      }\n";
            file << "    }";
            if (i < results.size() - 1) {
                file << ",";
            }
            file << "\n";
        }
        
        file << "  ],\n";
        file << "  \"timestamp\": \"" << GetCurrentTimestamp() << "\",\n";
        file << "  \"total_benchmarks\": " << results.size() << "\n";
        file << "}\n";
    }
    
    // ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
    void GenerateReport(const std::vector<BenchmarkResult>& results) {
        std::cout << "\n" << std::string(80, '=') << "\n";
        std::cout << "BENCHMARK RESULTS SUMMARY\n";
        std::cout << std::string(80, '=') << "\n";
        
        std::cout << std::left << std::setw(40) << "Benchmark"
                  << std::setw(15) << "Mean (ms)"
                  << std::setw(15) << "Std Dev (ms)"
                  << std::setw(10) << "Iterations" << "\n";
        std::cout << std::string(80, '-') << "\n";
        
        for (const auto& result : results) {
            std::cout << std::left << std::setw(40) << result.name
                      << std::setw(15) << std::fixed << std::setprecision(3) << (result.mean_time / 1e6)
                      << std::setw(15) << std::fixed << std::setprecision(3) << (result.std_dev_time / 1e6)
                      << std::setw(10) << result.iterations << "\n";
        }
        
        std::cout << std::string(80, '=') << "\n";
    }
    
private:
    std::string GetCurrentTimestamp() {
        auto now = std::chrono::system_clock::now();
        auto time_t = std::chrono::system_clock::to_time_t(now);
        std::stringstream ss;
        ss << std::put_time(std::gmtime(&time_t), "%Y-%m-%d %H:%M:%S UTC");
        return ss.str();
    }
};
```

### 7. ä¸»åŸºå‡†æµ‹è¯•ç¨‹åº

```cpp
// main_benchmark.cpp
#include "BenchmarkSuite.h"
#include "IndicatorBenchmarks.h"
#include "StrategyBenchmarks.h"
#include "MemoryBenchmarks.h"
#include "ComparisonBenchmarks.h"

int main(int argc, char* argv[]) {
    BenchmarkSuite suite("backtrader_cpp_benchmarks.json");
    
    // æŒ‡æ ‡æ€§èƒ½æµ‹è¯•
    std::cout << "Adding indicator benchmarks...\n";
    
    // ä¸åŒæ•°æ®è§„æ¨¡çš„SMAæµ‹è¯•
    for (size_t data_size : {1000, 10000, 100000, 1000000}) {
        for (size_t period : {10, 20, 50}) {
            suite.AddBenchmark<SMABenchmark>(period, data_size);
            suite.AddBenchmark<EMABenchmark>(period, data_size);
        }
    }
    
    // å¤æ‚æŒ‡æ ‡æµ‹è¯•
    for (size_t data_size : {10000, 100000, 1000000}) {
        suite.AddBenchmark<ComplexIndicatorBenchmark>(data_size);
    }
    
    // ç­–ç•¥æ€§èƒ½æµ‹è¯•
    std::cout << "Adding strategy benchmarks...\n";
    
    for (size_t data_size : {1000, 10000, 100000}) {
        suite.AddBenchmark<SimpleStrategyBenchmark>(data_size);
        
        for (size_t strategy_count : {1, 5, 10}) {
            suite.AddBenchmark<ComplexStrategyBenchmark>(data_size, strategy_count);
        }
    }
    
    // å†…å­˜å’Œç¼“å­˜æµ‹è¯•
    std::cout << "Adding memory benchmarks...\n";
    
    for (size_t data_size : {10000, 100000, 1000000, 10000000}) {
        suite.AddBenchmark<MemoryAccessBenchmark>(data_size);
        suite.AddBenchmark<CacheEfficiencyBenchmark>(data_size);
    }
    
    // Python vs C++ å¯¹æ¯”æµ‹è¯•
    std::cout << "Adding comparison benchmarks...\n";
    
    for (size_t data_size : {1000, 10000, 50000}) {
        suite.AddBenchmark<PythonVsCppBenchmark>("SMAStrategy", data_size);
    }
    
    // è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•
    auto results = suite.RunAll();
    
    // ç”ŸæˆæŠ¥å‘Š
    suite.GenerateReport(results);
    
    std::cout << "\nBenchmark results saved to: backtrader_cpp_benchmarks.json\n";
    
    return 0;
}
```

### 8. CI/CDé›†æˆ

#### GitHub ActionsåŸºå‡†æµ‹è¯•å·¥ä½œæµ

```yaml
# .github/workflows/benchmarks.yml
name: Performance Benchmarks

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    - cron: '0 2 * * 0'  # æ¯å‘¨æ—¥è¿è¡Œ

jobs:
  benchmark:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y cmake ninja-build libbenchmark-dev python3 python3-pip
        pip3 install backtrader pandas numpy
    
    - name: Configure CMake
      run: |
        cmake -B build -DCMAKE_BUILD_TYPE=Release -DBUILD_BENCHMARKS=ON -G Ninja
    
    - name: Build
      run: cmake --build build --config Release
    
    - name: Run benchmarks
      run: |
        cd build
        ./benchmarks/backtrader_benchmarks --benchmark_format=json > benchmark_results.json
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results
        path: build/benchmark_results.json
    
    - name: Compare with baseline
      if: github.event_name == 'pull_request'
      run: |
        python3 scripts/compare_benchmarks.py \
          --baseline baseline_benchmarks.json \
          --current build/benchmark_results.json \
          --output comparison.md
    
    - name: Comment PR with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const comparison = fs.readFileSync('comparison.md', 'utf8');
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comparison
          });
```

è¿™ä¸ªåŸºå‡†æµ‹è¯•æ¡†æ¶æä¾›äº†ï¼š

1. **å…¨é¢çš„æ€§èƒ½æµ‹è¯•**: è¦†ç›–æ‰€æœ‰å…³é”®ç»„ä»¶å’Œåœºæ™¯
2. **ç»Ÿè®¡å­¦å‡†ç¡®æ€§**: è‡ªé€‚åº”è¿­ä»£æ¬¡æ•°å’Œè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
3. **èµ„æºç›‘æ§**: CPUå’Œå†…å­˜ä½¿ç”¨ç‡è·Ÿè¸ª
4. **å¯¹æ¯”æµ‹è¯•**: Python vs C++æ€§èƒ½å¯¹æ¯”
5. **CI/CDé›†æˆ**: è‡ªåŠ¨åŒ–æ€§èƒ½å›å½’æ£€æµ‹
6. **è¯¦ç»†æŠ¥å‘Š**: JSONå’Œæ–‡æœ¬æ ¼å¼çš„ç»“æœè¾“å‡º

é€šè¿‡è¿™ä¸ªæ¡†æ¶ï¼Œå¯ä»¥ç²¾ç¡®éªŒè¯C++é‡æ„ç‰ˆæœ¬æ˜¯å¦è¾¾åˆ°äº†8-15å€çš„æ€§èƒ½æå‡ç›®æ ‡ã€‚